{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_url</th>\n",
       "      <th>permalink</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>created</th>\n",
       "      <th>n_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>https://i.redd.it/djxxrwhy03841.jpg</td>\n",
       "      <td>/r/curlyhair/comments/eid0mc/it_took_me_the_en...</td>\n",
       "      <td>It took me the entire dang decade to learn to ...</td>\n",
       "      <td>bennyque</td>\n",
       "      <td>2019-12-31 21:57:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>https://i.redd.it/qublkc1ci4841.jpg</td>\n",
       "      <td>/r/curlyhair/comments/eifv6w/transitioning_fro...</td>\n",
       "      <td>Transitioning from dreadlocks to natural hair....</td>\n",
       "      <td>koalaTC</td>\n",
       "      <td>2020-01-01 02:56:13</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>https://i.redd.it/ia88iag4q5841.jpg</td>\n",
       "      <td>/r/curlyhair/comments/eiho5r/making_good_progr...</td>\n",
       "      <td>Making good progress! After 2 weeks of curlygirl!</td>\n",
       "      <td>a-palegal</td>\n",
       "      <td>2020-01-01 07:01:41</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>https://i.redd.it/uli9ijxvk6841.jpg</td>\n",
       "      <td>/r/curlyhair/comments/eij3pv/had_a_very_weird_...</td>\n",
       "      <td>Had a very weird, but very welcome hair transf...</td>\n",
       "      <td>saad_sunday</td>\n",
       "      <td>2020-01-01 09:54:05</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>https://i.redd.it/umht8y6sx6841.jpg</td>\n",
       "      <td>/r/curlyhair/comments/eijvnu/december_2018_whe...</td>\n",
       "      <td>december 2018 (when i thought i had straight h...</td>\n",
       "      <td>jaanwar17</td>\n",
       "      <td>2020-01-01 11:06:22</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>https://i.redd.it/miex8ej6z6841.jpg</td>\n",
       "      <td>/r/curlyhair/comments/eijytv/reflecting_in_the...</td>\n",
       "      <td>Reflecting in the new decade</td>\n",
       "      <td>gettingfiscal</td>\n",
       "      <td>2020-01-01 11:14:10</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>https://i.redd.it/bcqvg4tu88841.jpg</td>\n",
       "      <td>/r/curlyhair/comments/einav1/had_to_show_my_de...</td>\n",
       "      <td>Had to show my decade pic.. This was me in 200...</td>\n",
       "      <td>Curly_carpenter</td>\n",
       "      <td>2020-01-01 15:30:10</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>https://i.redd.it/4me08ersz8841.png</td>\n",
       "      <td>/r/curlyhair/comments/eip941/did_i_do_the_big_...</td>\n",
       "      <td>Did I do the big chop right?</td>\n",
       "      <td>becky18567</td>\n",
       "      <td>2020-01-01 18:01:14</td>\n",
       "      <td>282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>https://i.redd.it/s6okctcp69841.jpg</td>\n",
       "      <td>/r/curlyhair/comments/eiprhk/its_not_much_but_...</td>\n",
       "      <td>It’s not much, but it’s honest work. (2 weeks ...</td>\n",
       "      <td>hcll</td>\n",
       "      <td>2020-01-01 18:39:54</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>https://i.redd.it/qhdec9dp0a841.jpg</td>\n",
       "      <td>/r/curlyhair/comments/eiruy5/finally_big_chopp...</td>\n",
       "      <td>Finally big chopped (: Goodbye bleach</td>\n",
       "      <td>BecomingBlake</td>\n",
       "      <td>2020-01-01 21:28:05</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             image_url  \\\n",
       "0  https://i.redd.it/djxxrwhy03841.jpg   \n",
       "1  https://i.redd.it/qublkc1ci4841.jpg   \n",
       "2  https://i.redd.it/ia88iag4q5841.jpg   \n",
       "3  https://i.redd.it/uli9ijxvk6841.jpg   \n",
       "4  https://i.redd.it/umht8y6sx6841.jpg   \n",
       "5  https://i.redd.it/miex8ej6z6841.jpg   \n",
       "6  https://i.redd.it/bcqvg4tu88841.jpg   \n",
       "7  https://i.redd.it/4me08ersz8841.png   \n",
       "8  https://i.redd.it/s6okctcp69841.jpg   \n",
       "9  https://i.redd.it/qhdec9dp0a841.jpg   \n",
       "\n",
       "                                           permalink  \\\n",
       "0  /r/curlyhair/comments/eid0mc/it_took_me_the_en...   \n",
       "1  /r/curlyhair/comments/eifv6w/transitioning_fro...   \n",
       "2  /r/curlyhair/comments/eiho5r/making_good_progr...   \n",
       "3  /r/curlyhair/comments/eij3pv/had_a_very_weird_...   \n",
       "4  /r/curlyhair/comments/eijvnu/december_2018_whe...   \n",
       "5  /r/curlyhair/comments/eijytv/reflecting_in_the...   \n",
       "6  /r/curlyhair/comments/einav1/had_to_show_my_de...   \n",
       "7  /r/curlyhair/comments/eip941/did_i_do_the_big_...   \n",
       "8  /r/curlyhair/comments/eiprhk/its_not_much_but_...   \n",
       "9  /r/curlyhair/comments/eiruy5/finally_big_chopp...   \n",
       "\n",
       "                                                text           author  \\\n",
       "0  It took me the entire dang decade to learn to ...         bennyque   \n",
       "1  Transitioning from dreadlocks to natural hair....          koalaTC   \n",
       "2  Making good progress! After 2 weeks of curlygirl!        a-palegal   \n",
       "3  Had a very weird, but very welcome hair transf...      saad_sunday   \n",
       "4  december 2018 (when i thought i had straight h...        jaanwar17   \n",
       "5                       Reflecting in the new decade    gettingfiscal   \n",
       "6  Had to show my decade pic.. This was me in 200...  Curly_carpenter   \n",
       "7                       Did I do the big chop right?       becky18567   \n",
       "8  It’s not much, but it’s honest work. (2 weeks ...             hcll   \n",
       "9              Finally big chopped (: Goodbye bleach    BecomingBlake   \n",
       "\n",
       "               created n_comments  \n",
       "0  2019-12-31 21:57:00          1  \n",
       "1  2020-01-01 02:56:13          4  \n",
       "2  2020-01-01 07:01:41          1  \n",
       "3  2020-01-01 09:54:05          3  \n",
       "4  2020-01-01 11:06:22         10  \n",
       "5  2020-01-01 11:14:10         36  \n",
       "6  2020-01-01 15:30:10         11  \n",
       "7  2020-01-01 18:01:14        282  \n",
       "8  2020-01-01 18:39:54         14  \n",
       "9  2020-01-01 21:28:05          1  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curly_df = pd.read_csv('curlyhair.csv')\n",
    "\n",
    "curly_df = curly_df[curly_df['flair'] == 'before and after']\n",
    "curly_df = curly_df.drop(['sub_id', 'subreddit','flair'], axis=1)\n",
    "curly_df.index = range(len(curly_df))\n",
    "\n",
    "curly_df.head(10)\n",
    "#len(curly_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  text         author  \\\n",
      "0    Routine: ion swimming shampoo, Pacifica pineap...      somniumni   \n",
      "1    current routine:\\nwash every 4-6 days with ogx...         cxsmiq   \n",
      "2    Thank you for taking the time to write such an...     S4alishow8   \n",
      "3    im literally just not shampooing, my routine i...   pancakeman49   \n",
      "4    Current routine: wash 2x/week with Shea Moistu...       bennyque   \n",
      "..                                                 ...            ...   \n",
      "838  Routine: wash and condition with Bounce Curl (...     daylightxx   \n",
      "839  Laziness.\\n\\nHonestly my routine at this point...     Lolihumper   \n",
      "840  My routine of washing and taking care of my ha...  Stupidbimbo98   \n",
      "841                              Routine: i do nothing    raquelmorge   \n",
      "842  Here is my routine... Well, I don't really hav...         IBentu   \n",
      "\n",
      "                 created  is_subm  \\\n",
      "0    2019-12-31 19:19:24     True   \n",
      "1    2019-12-31 20:07:07     True   \n",
      "2    2019-12-31 20:47:28     True   \n",
      "3    2019-12-31 21:10:27     True   \n",
      "4    2019-12-31 22:05:17     True   \n",
      "..                   ...      ...   \n",
      "838  2020-01-16 20:05:39     True   \n",
      "839  2020-01-16 20:15:22     True   \n",
      "840  2020-01-16 20:27:30     True   \n",
      "841  2020-01-16 21:01:29     True   \n",
      "842  2020-01-16 23:05:44     True   \n",
      "\n",
      "                                             permalink  \n",
      "0    /r/curlyhair/comments/eib80b/couldve_hoped_for...  \n",
      "1    /r/curlyhair/comments/eibt1s/i_dont_understand...  \n",
      "2    /r/curlyhair/comments/ehpmg6/the_base_of_my_ha...  \n",
      "3    /r/curlyhair/comments/eici5z/rn_im_just_leavin...  \n",
      "4    /r/curlyhair/comments/eid0mc/it_took_me_the_en...  \n",
      "..                                                 ...  \n",
      "838  /r/curlyhair/comments/epsu9v/ive_past_the_year...  \n",
      "839  /r/curlyhair/comments/epqvv8/so_i_tried_condit...  \n",
      "840  /r/curlyhair/comments/epsyhc/i_have_a_question...  \n",
      "841  /r/curlyhair/comments/ept82q/can_someone_tell_...  \n",
      "842  /r/curlyhair/comments/epv2ws/been_getting_back...  \n",
      "\n",
      "[843 rows x 5 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joanna/anaconda2/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:6: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "com_df = pd.read_csv('comments_curlyhair.csv')\n",
    "com_df = com_df.drop(['sub_id', 'parent_id','subreddit'], axis=1)\n",
    "com_df = com_df[com_df['is_subm'] == True]\n",
    "\n",
    "mask = com_df['text'].str.contains('routine', na=True, case=False)\n",
    "com_df = comments_df[mask]\n",
    "com_df.index = range(len(com_df))\n",
    "\n",
    "#com_df.head(10)\n",
    "print(com_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
