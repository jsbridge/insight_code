{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import csv\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import re\n",
    "import unicodedata\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits = ['curlyhair']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get posts and image urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# https://www.unixtimestamp.com/index.php\n",
    "# before and after dates\n",
    "after =  \"1464480000\" # 5/29/2016 Earliest post with flair data, that I can tell\n",
    "before = \"1600000000\"  # 9/13/2020\n",
    "\n",
    "def getPushshiftData(query, after, before, sub):\n",
    "    url = 'https://api.pushshift.io/reddit/submission/search/?after='+str(after)+'&before='+str(before)+'&subreddit='+str(sub)\n",
    "    print(url)\n",
    "    \n",
    "    r = requests.get(url)\n",
    "    data = json.loads(r.text)\n",
    "    return data['data']\n",
    "\n",
    "def collectSubData(subm):\n",
    "    subData = list() #list to store data points\n",
    "    title = subm['title']  \n",
    "    author = subm['author']\n",
    "    \n",
    "    sub_id = subm['id']\n",
    "    url = subm['url']\n",
    "    subreddit = subm['subreddit']\n",
    "    created = datetime.datetime.fromtimestamp(subm['created_utc'])\n",
    "    numComms = subm['num_comments']\n",
    "    #print(subm)\n",
    "    try:\n",
    "        flair = subm['link_flair_text']\n",
    "    except KeyError:\n",
    "        flair = np.nan\n",
    "    perm = subm['permalink']\n",
    "                 \n",
    "    subData.append((sub_id,url,perm,title,author,subreddit,created,numComms, flair))\n",
    "    subStats[sub_id] = subData\n",
    "\n",
    "for sub in subreddits:\n",
    "    \n",
    "    print(sub)\n",
    "\n",
    "    # https://www.unixtimestamp.com/index.php\n",
    "    #before and after dates\n",
    "    query = \"\"\n",
    "    subCount = 0\n",
    "    subStats = {}\n",
    "\n",
    "    data = getPushshiftData(query, after, before, sub)\n",
    "    # Will run until all posts have been gathered \n",
    "    # from the 'after' date up until before date\n",
    "    while len(data) > 0:\n",
    "        for submission in data:\n",
    "            collectSubData(submission)\n",
    "            subCount+=1\n",
    "        after = data[-1]['created_utc']\n",
    "        try:\n",
    "            data = getPushshiftData(query, after, before, sub)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    #print(len(data))\n",
    "    \n",
    "    def updateSubs_file(filename):\n",
    "        upload_count = 0\n",
    "        location = \"./\"\n",
    "        file = location + filename + '.csv'\n",
    "        with open(file, 'a', newline='', encoding='utf-8') as file: \n",
    "            a = csv.writer(file, delimiter=',')\n",
    "            headers = ['sub_id','image_url','permalink','text','author','subreddit','created','n_comments','flair']\n",
    "            a.writerow(headers)\n",
    "            for sub in subStats:\n",
    "                a.writerow(subStats[sub][0])\n",
    "                upload_count+=1\n",
    "\n",
    "            print(str(upload_count) + \" submissions have been uploaded\")\n",
    "\n",
    "    updateSubs_file(sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.unixtimestamp.com/index.php\n",
    "# before and after dates\n",
    "after = \"1464480000\" # 5/29/2016 Earliest post with flair data, that I can tell\n",
    "before = \"1600000000\"  # 9/13/2020\n",
    "\n",
    "def getPushshiftData(query, after, before, sub):\n",
    "    url = 'https://api.pushshift.io/reddit/search/comment/?size=1000&after='+str(after)+'&before='+str(before)+'&subreddit='+str(sub)\n",
    "    print(url)\n",
    "    \n",
    "    r = requests.get(url)\n",
    "    data = json.loads(r.text)\n",
    "    return data['data']\n",
    "\n",
    "def collectSubData(subm):\n",
    "    subData = list() #list to store data points\n",
    "    title = subm['body']  \n",
    "    author = subm['author']\n",
    "    sub_id = subm['id']\n",
    "    parent_id = subm['parent_id']\n",
    "    subreddit = subm['subreddit']\n",
    "    created = datetime.datetime.fromtimestamp(subm['created_utc'])\n",
    "    link_id = subm['link_id']\n",
    "    #print(subm)\n",
    "    try:\n",
    "        submitter= subm['is_submitter']\n",
    "    except KeyError:\n",
    "        submitter = np.nan\n",
    "    try:\n",
    "        perm = subm['permalink']\n",
    "    except KeyError:\n",
    "        perm = np.nan\n",
    "    \n",
    "    subData.append((sub_id,link_id,parent_id,title,author,subreddit,created,submitter, perm))\n",
    "    subStats[sub_id] = subData\n",
    "\n",
    "for sub in subreddits:\n",
    "    \n",
    "    print(sub)\n",
    "\n",
    "    query = \"\"\n",
    "    subCount = 0\n",
    "    subStats = {}\n",
    "\n",
    "    data = getPushshiftData(query, after, before, sub)\n",
    "    # Will run until all posts have been gathered \n",
    "    # from the 'after' date up until before date\n",
    "    while len(data) > 0:\n",
    "        for submission in data:\n",
    "            collectSubData(submission)\n",
    "            subCount+=1\n",
    "        after = data[-1]['created_utc']\n",
    "        data = getPushshiftData(query, after, before, sub)\n",
    "\n",
    "    #print(len(data))\n",
    "\n",
    "    def updateSubs_file(filename):\n",
    "        upload_count = 0\n",
    "        location = \"./\"\n",
    "        fil = location + 'comments_' + filename + '.csv'\n",
    "        with open(fil, 'w', newline='', encoding='utf-8') as fil: \n",
    "            a = csv.writer(fil, delimiter=',')\n",
    "            headers = ['sub_id','link_id','parent_id','text','author','subreddit','created','is_subm','permalink']\n",
    "            a.writerow(headers)\n",
    "            for sub in subStats:\n",
    "                a.writerow(subStats[sub][0])\n",
    "                upload_count+=1\n",
    "\n",
    "            print(str(upload_count) + \" submissions have been downloaded\")\n",
    "\n",
    "    updateSubs_file(sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Didn't end up using this module\n",
    "\n",
    "#import image_scraper\n",
    "#import pandas as pd\n",
    "\n",
    "#curly_df = pd.read_csv('curlyhair.csv')\n",
    "\n",
    "#curly_df = curly_df[curly_df['flair'] == 'before and after']\n",
    "#curly_urls = curly_df['image_url']\n",
    "\n",
    "#for url in curly_urls:\n",
    "#    try:\n",
    "#        image_scraper.scrape_images(url --dump-urls )\n",
    "#    except PageLoadError:\n",
    "#        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image \n",
    "import os, io, hashlib\n",
    "import pandas as pd\n",
    "\n",
    "# This chunk of code from https://towardsdatascience.com/image-scraping-with-python-a96feda8af2d\n",
    "# but augmented for my purposes\n",
    "\n",
    "def persist_image(folder_path:str,url:str):\n",
    "    try:\n",
    "        image_content = requests.get(url).content\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR - Could not download {url} - {e}\")\n",
    "\n",
    "    try:\n",
    "        image_file = io.BytesIO(image_content)\n",
    "        image = Image.open(image_file).convert('RGB')\n",
    "        file_path = os.path.join(folder_path,hashlib.sha1(image_content).hexdigest()[:10] + '.jpg')\n",
    "        with open(file_path, 'wb') as f:\n",
    "            image.save(f, \"JPEG\", quality=85)\n",
    "        if os.path.getsize(file_path) < 5000:\n",
    "            os.remove(file_path)\n",
    "        else:\n",
    "            print(f\"SUCCESS - saved {url} - as {file_path}\")\n",
    "            return file_path\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR - Could not save {url} - {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curly_df = pd.read_csv('curlyhair.csv')\n",
    "\n",
    "curly_df = curly_df[curly_df['flair'] == 'hair victory']\n",
    "curly_urls = curly_df['image_url']\n",
    "\n",
    "f = open('hair_images/image_urls.dat', 'w')\n",
    "f.write('#           image_url                         file_path\\n')\n",
    "\n",
    "for url in curly_urls:\n",
    "    file_path = persist_image('./hair_images/', url)\n",
    "    if file_path != None:\n",
    "        f.write(f'{url}    {file_path}\\n')\n",
    "    else:\n",
    "        f.write(f'{url}    not downloaded\\n')\n",
    "f.close()\n",
    "\n",
    "# This doesn't work with imgur hosted files, and sometimes there are \"comment\" posts which \n",
    "# don't have images anyway, so big deal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
