{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator \n",
    "from keras.models import Sequential \n",
    "from keras.layers import Conv2D, MaxPooling2D \n",
    "from keras.layers import Activation, Dropout, Flatten, Dense \n",
    "from keras import backend as K "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width, img_height = 400, 400\n",
    "  \n",
    "train_data_dir = './Figaro1k/Original/Training'\n",
    "validation_data_dir = './Figaro1k/Original/Testing'\n",
    "nb_train_samples = 480 \n",
    "nb_validation_samples = 120\n",
    "epochs = 50\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "if K.image_data_format() == 'channels_first': \n",
    "    input_shape = (3, img_width, img_height) \n",
    "else: \n",
    "    input_shape = (img_width, img_height, 3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(4))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 480 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Epoch 1/50\n",
      "30/30 [==============================] - 54s 2s/step - loss: 2.0464 - accuracy: 0.2521 - val_loss: 1.3711 - val_accuracy: 0.2857\n",
      "Epoch 2/50\n",
      "30/30 [==============================] - 51s 2s/step - loss: 1.4020 - accuracy: 0.3042 - val_loss: 1.5842 - val_accuracy: 0.2308\n",
      "Epoch 3/50\n",
      "30/30 [==============================] - 50s 2s/step - loss: 1.3559 - accuracy: 0.3729 - val_loss: 1.3293 - val_accuracy: 0.3462\n",
      "Epoch 4/50\n",
      "30/30 [==============================] - 50s 2s/step - loss: 1.3001 - accuracy: 0.3938 - val_loss: 1.2932 - val_accuracy: 0.3654\n",
      "Epoch 5/50\n",
      "30/30 [==============================] - 50s 2s/step - loss: 1.3302 - accuracy: 0.4062 - val_loss: 1.3696 - val_accuracy: 0.2788\n",
      "Epoch 6/50\n",
      "30/30 [==============================] - 50s 2s/step - loss: 1.2740 - accuracy: 0.4062 - val_loss: 1.2542 - val_accuracy: 0.3654\n",
      "Epoch 7/50\n",
      "30/30 [==============================] - 50s 2s/step - loss: 1.2951 - accuracy: 0.4083 - val_loss: 1.7788 - val_accuracy: 0.3462\n",
      "Epoch 8/50\n",
      "30/30 [==============================] - 51s 2s/step - loss: 1.2171 - accuracy: 0.4292 - val_loss: 1.0761 - val_accuracy: 0.3846\n",
      "Epoch 9/50\n",
      "30/30 [==============================] - 50s 2s/step - loss: 1.2264 - accuracy: 0.4604 - val_loss: 1.4047 - val_accuracy: 0.3750\n",
      "Epoch 10/50\n",
      "30/30 [==============================] - 50s 2s/step - loss: 1.2887 - accuracy: 0.4708 - val_loss: 1.1186 - val_accuracy: 0.4231\n",
      "Epoch 11/50\n",
      "30/30 [==============================] - 50s 2s/step - loss: 1.1592 - accuracy: 0.4521 - val_loss: 1.5872 - val_accuracy: 0.3750\n",
      "Epoch 12/50\n",
      "30/30 [==============================] - 50s 2s/step - loss: 1.1385 - accuracy: 0.5188 - val_loss: 1.9754 - val_accuracy: 0.3077\n",
      "Epoch 13/50\n",
      "30/30 [==============================] - 54s 2s/step - loss: 1.1041 - accuracy: 0.5271 - val_loss: 2.0402 - val_accuracy: 0.3942\n",
      "Epoch 14/50\n",
      "30/30 [==============================] - 61s 2s/step - loss: 1.0736 - accuracy: 0.5271 - val_loss: 1.3409 - val_accuracy: 0.3462\n",
      "Epoch 15/50\n",
      "30/30 [==============================] - 53s 2s/step - loss: 1.0603 - accuracy: 0.5437 - val_loss: 1.5425 - val_accuracy: 0.4423\n",
      "Epoch 16/50\n",
      "30/30 [==============================] - 53s 2s/step - loss: 1.0418 - accuracy: 0.5542 - val_loss: 1.0026 - val_accuracy: 0.4423\n",
      "Epoch 17/50\n",
      "30/30 [==============================] - 53s 2s/step - loss: 1.0017 - accuracy: 0.5688 - val_loss: 1.7040 - val_accuracy: 0.3929\n",
      "Epoch 18/50\n",
      "30/30 [==============================] - 54s 2s/step - loss: 1.0254 - accuracy: 0.5625 - val_loss: 1.0661 - val_accuracy: 0.4615\n",
      "Epoch 19/50\n",
      "30/30 [==============================] - 54s 2s/step - loss: 0.9808 - accuracy: 0.5583 - val_loss: 1.5318 - val_accuracy: 0.4519\n",
      "Epoch 20/50\n",
      "30/30 [==============================] - 55s 2s/step - loss: 0.9816 - accuracy: 0.5854 - val_loss: 1.3606 - val_accuracy: 0.3942\n",
      "Epoch 21/50\n",
      "30/30 [==============================] - 51s 2s/step - loss: 0.9780 - accuracy: 0.5896 - val_loss: 0.9169 - val_accuracy: 0.4327\n",
      "Epoch 22/50\n",
      "30/30 [==============================] - 50s 2s/step - loss: 0.9501 - accuracy: 0.5938 - val_loss: 1.1820 - val_accuracy: 0.5000\n",
      "Epoch 23/50\n",
      "30/30 [==============================] - 53s 2s/step - loss: 0.9739 - accuracy: 0.6000 - val_loss: 1.6074 - val_accuracy: 0.4615\n",
      "Epoch 24/50\n",
      "30/30 [==============================] - 53s 2s/step - loss: 0.8767 - accuracy: 0.6396 - val_loss: 2.8481 - val_accuracy: 0.4519\n",
      "Epoch 25/50\n",
      "30/30 [==============================] - 53s 2s/step - loss: 0.8698 - accuracy: 0.6583 - val_loss: 0.8936 - val_accuracy: 0.5089\n",
      "Epoch 26/50\n",
      "30/30 [==============================] - 50s 2s/step - loss: 0.8761 - accuracy: 0.6375 - val_loss: 0.8226 - val_accuracy: 0.4327\n",
      "Epoch 27/50\n",
      "30/30 [==============================] - 53s 2s/step - loss: 0.8388 - accuracy: 0.6604 - val_loss: 1.6869 - val_accuracy: 0.4519\n",
      "Epoch 28/50\n",
      "30/30 [==============================] - 51s 2s/step - loss: 0.8355 - accuracy: 0.6542 - val_loss: 2.3688 - val_accuracy: 0.4519\n",
      "Epoch 29/50\n",
      "30/30 [==============================] - 50s 2s/step - loss: 0.8314 - accuracy: 0.6521 - val_loss: 1.6520 - val_accuracy: 0.5096\n",
      "Epoch 30/50\n",
      "30/30 [==============================] - 50s 2s/step - loss: 0.8227 - accuracy: 0.6625 - val_loss: 0.8518 - val_accuracy: 0.5096\n",
      "Epoch 31/50\n",
      "30/30 [==============================] - 52s 2s/step - loss: 0.7892 - accuracy: 0.6646 - val_loss: 1.9187 - val_accuracy: 0.4423\n",
      "Epoch 32/50\n",
      "30/30 [==============================] - 54s 2s/step - loss: 0.7473 - accuracy: 0.7000 - val_loss: 0.8770 - val_accuracy: 0.4904\n",
      "Epoch 33/50\n",
      "30/30 [==============================] - 54s 2s/step - loss: 0.7284 - accuracy: 0.6979 - val_loss: 1.6210 - val_accuracy: 0.4732\n",
      "Epoch 34/50\n",
      "30/30 [==============================] - 52s 2s/step - loss: 0.8260 - accuracy: 0.6896 - val_loss: 1.1728 - val_accuracy: 0.5192\n",
      "Epoch 35/50\n",
      "30/30 [==============================] - 52s 2s/step - loss: 0.6923 - accuracy: 0.7146 - val_loss: 1.4991 - val_accuracy: 0.5288\n",
      "Epoch 36/50\n",
      "30/30 [==============================] - 69s 2s/step - loss: 0.7134 - accuracy: 0.7229 - val_loss: 1.0619 - val_accuracy: 0.5481\n",
      "Epoch 37/50\n",
      "15/30 [==============>...............] - ETA: 27s - loss: 0.6067 - accuracy: 0.7542"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator( \n",
    "    rescale=1. / 255, \n",
    "    shear_range=0.2, \n",
    "    zoom_range=0.2, \n",
    "    horizontal_flip=True) \n",
    "  \n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255) \n",
    "  \n",
    "train_generator = train_datagen.flow_from_directory( \n",
    "    train_data_dir, \n",
    "    target_size=(img_width, img_height), \n",
    "    batch_size=batch_size, \n",
    "    class_mode='categorical') \n",
    "  \n",
    "validation_generator = test_datagen.flow_from_directory( \n",
    "    validation_data_dir, \n",
    "    target_size=(img_width, img_height), \n",
    "    batch_size=batch_size, \n",
    "    class_mode='categorical') \n",
    "  \n",
    "model.fit_generator( \n",
    "    train_generator, \n",
    "    steps_per_epoch=nb_train_samples // batch_size, \n",
    "    epochs=epochs, \n",
    "    validation_data=validation_generator, \n",
    "    validation_steps=nb_validation_samples // batch_size) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('model_saved.h5') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'curly': 0, 'kinky': 1, 'straight': 2, 'wavy': 3}\n"
     ]
    }
   ],
   "source": [
    "classes = train_generator.class_indices\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[30  0  0  0]\n",
      " [30  0  0  0]\n",
      " [30  0  0  0]\n",
      " [30  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (3, img_width, img_height)\n",
    "else:\n",
    "    input_shape = (img_width, img_height, 3)\n",
    "\n",
    "#compile the 3-layer CNN\n",
    "model = Sequential() \n",
    "model.add(Conv2D(32, (2, 2), input_shape = input_shape)) \n",
    "model.add(Activation('relu')) \n",
    "model.add(MaxPooling2D(pool_size =(2, 2))) \n",
    "  \n",
    "model.add(Conv2D(32, (2, 2))) \n",
    "model.add(Activation('relu')) \n",
    "model.add(MaxPooling2D(pool_size =(2, 2))) \n",
    "  \n",
    "model.add(Conv2D(64, (2, 2))) \n",
    "model.add(Activation('relu')) \n",
    "model.add(MaxPooling2D(pool_size =(2, 2))) \n",
    "  \n",
    "model.add(Flatten()) \n",
    "model.add(Dense(64)) \n",
    "model.add(Activation('relu')) \n",
    "model.add(Dropout(0.5)) \n",
    "model.add(Dense(1)) \n",
    "model.add(Activation('sigmoid')) \n",
    "  \n",
    "model.compile(loss ='binary_crossentropy', \n",
    "                     optimizer ='rmsprop', \n",
    "                   metrics =['accuracy']) \n",
    "\n",
    "#load weights into new model\n",
    "model.load_weights('model_saved.h5')\n",
    "\n",
    "\n",
    "#GENERATE CONFUSION MATRIX, ITERATE OVER ALL TEST IMAGES\n",
    "#path to folder where test images are located\n",
    "path = './Figaro1k/Original/Testing/'\n",
    "files = os.listdir(path)\n",
    "\n",
    "y_true = [] #list of ground truth labels, will be plugged into sklearn confusion matrix\n",
    "y_pred = [] #list of predicted labels\n",
    "dict_labels = {0: 'curly', 1: 'kinky', 2: 'straight', 3: 'wavy'}\n",
    "\n",
    "#iterate over every subfolder, and every image in each subfolder\n",
    "#subfolders correspond to each class\n",
    "for file in files:\n",
    "    if os.path.isdir(os.path.join(path, file)):\n",
    "        label_true = file #label all files in this folder as name of folder\n",
    "        path_subfolder = path + file\n",
    "        subfiles = os.listdir(path_subfolder) #list of all files in subfolder\n",
    "        for subfile in subfiles:\n",
    "            if subfile == '.DS_Store':\n",
    "                continue\n",
    "            img_path = os.path.join(path_subfolder, subfile)\n",
    "            #load image\n",
    "            img = image.load_img(img_path, target_size = (img_width, img_height))\n",
    "            img_tensor = image.img_to_array(img)\n",
    "            img_tensor = np.expand_dims(img_tensor, axis=0)\n",
    "            img_tensor /= 255\n",
    "\n",
    "            pred = model.predict(img_tensor) #make prediction\n",
    "            label_pred = dict_labels[np.argmax(pred[0])]\n",
    "\n",
    "            y_true.append(label_true) #append true label\n",
    "            y_pred.append(label_pred) #append predicted label\n",
    "\n",
    "\n",
    "#generate, print the confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred, labels=['curly','kinky','straight', 'wavy'])\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
