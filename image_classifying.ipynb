{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
    "import pymeanshift as pms\n",
    "import os, sys\n",
    "from glob import glob\n",
    "from PIL import Image \n",
    "import keras\n",
    "import tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resize images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imgs = glob('./hair_images/*')\n",
    "\n",
    "# def resize(imgs):\n",
    "#     for img in imgs:\n",
    "#         filename = img.split('/')[2]\n",
    "#         filename = filename.split('.')[0]\n",
    "#         if filename == 'image_urls':\n",
    "#             continue\n",
    "#         if os.path.exists('./resized_hair_images/'+filename+'_resized.jpg') == True:\n",
    "#             continue\n",
    "#         im = Image.open(img)\n",
    "#         imResize = im.resize((300,400), Image.ANTIALIAS)\n",
    "#         imResize.save('./resized_hair_images/'+filename+'_resized.jpg', 'JPEG', quality=90)\n",
    "\n",
    "# resize(imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find hair via contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_YCrCb = np.array([0,133,77],np.uint8)\n",
    "# max_YCrCb = np.array([255,173,127],np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = cv2.imread('wavy.jpg')\n",
    "\n",
    "# imageYCrCb = cv2.cvtColor(img,cv2.COLOR_BGR2YCR_CB)\n",
    "# skinRegion = cv2.inRange(imageYCrCb,min_YCrCb,max_YCrCb)\n",
    "\n",
    "# contours, hierarchy = cv2.findContours(skinRegion, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# # Draw the contour on the source image\n",
    "# for i, c in enumerate(contours):\n",
    "#     area = cv2.contourArea(c)\n",
    "#     if area > 9000:\n",
    "#         cv2.drawContours(img, contours, i, (0, 255, 0), 3)\n",
    "\n",
    "# # Display the source image\n",
    "# cv2.imwrite('output1.jpg', img)\n",
    "# plt.figure(figsize=(12,12))\n",
    "# plt.imshow(img[:,:,::-1])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# face_cascade = cv2.CascadeClassifier('/Users/joanna/anaconda2/envs/py37/lib/python3.7/site-packages/cv2/data/haarcascade_frontalface_default.xml')\n",
    "# eye_cascade = cv2.CascadeClassifier('/Users/joanna/anaconda2/envs/py37/lib/python3.7/site-packages/cv2/data/haarcascade_eye.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = cv2.imread('hair.jpg')\n",
    "# gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# faces = face_cascade.detectMultiScale(gray, scaleFactor=1.5, minNeighbors=3)\n",
    "\n",
    "# for (x,y,w,h) in faces:\n",
    "#     cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "#     roi_gray = gray[y:y+h, x:x+w]\n",
    "#     roi_color = img[y:y+h, x:x+w]\n",
    "#     eyes = eye_cascade.detectMultiScale(roi_gray,minSize=(100, 100), maxSize=(150,150))\n",
    "#     for (ex,ey,ew,eh) in eyes:\n",
    "#         cv2.rectangle(roi_color,(ex,ey),(ex+ew,ey+eh),(0,255,0),2)\n",
    "\n",
    "# cv2.imwrite('output2.jpg', img)\n",
    "# plt.figure(figsize=(12,12))\n",
    "# plt.imshow(img[:,:,::-1])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = cv2.imread('hair.jpg')\n",
    "# img = cv2.cvtColor(img ,cv2.COLOR_BGR2GRAY)\n",
    "# cv2.imwrite('blackwhite.jpg', img)\n",
    "\n",
    "# (segmented_image, labels_image, number_regions) = pms.segment(img, spatial_radius=6, \n",
    "#                                                               range_radius=4.5, min_density=1500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(12,12))\n",
    "# plt.imshow(segmented_image)\n",
    "# plt.show()\n",
    "\n",
    "# cv2.imwrite('blackwhite_seg.jpg', segmented_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing with edge detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import cv2\n",
    "# import numpy as np\n",
    "# from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = cv2.imread('hair_images/903ff49749.jpg',0)\n",
    "# edges = cv2.Canny(img,100,250)\n",
    "\n",
    "# plt.figure(figsize=(20,20))\n",
    "# plt.subplot(121),plt.imshow(img,cmap = 'gray')\n",
    "# plt.title('Original Image'), plt.xticks([]), plt.yticks([])\n",
    "# plt.subplot(122),plt.imshow(edges,cmap = 'gray')\n",
    "# plt.title('Edge Image'), plt.xticks([]), plt.yticks([])\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def auto_canny(image, sigma=0.33):\n",
    "# \t# compute the median of the single channel pixel intensities\n",
    "# \tv = np.median(image)\n",
    "     \n",
    "# \t# apply automatic Canny edge detection using the computed median\n",
    "# \tlower = int(max(0, (1.0 - sigma) * v))\n",
    "# \tupper = int(min(255, (1.0 + sigma) * v))\n",
    "# \tedged = cv2.Canny(image, lower, upper)\n",
    " \n",
    "# \t# return the edged image\n",
    "# \treturn edged\n",
    "\n",
    "# imagePath = 'hair_images/903ff49749.jpg'\n",
    "# image = cv2.imread(imagePath)\n",
    "# gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "# blurred = cv2.GaussianBlur(gray, (3, 3), 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "# blurred = cv2.GaussianBlur(gray, (3, 3), 0)\n",
    " \n",
    "# # apply Canny edge detection using a wide threshold, tight\n",
    "# # threshold, and automatically determined threshold\n",
    "# wide = cv2.Canny(blurred, 10, 200)\n",
    "# tight = cv2.Canny(blurred, 225, 250)\n",
    "# auto = auto_canny(blurred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(20,20))\n",
    "# plt.subplot(121),plt.imshow(img,cmap = 'gray')\n",
    "# plt.title('Original Image'), plt.xticks([]), plt.yticks([])\n",
    "# plt.subplot(122),plt.imshow(wide,cmap = 'gray')\n",
    "# plt.title('Edge Image'), plt.xticks([]), plt.yticks([])\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying masked Reddit images via CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/joanna/anaconda2/envs/py37/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/joanna/anaconda2/envs/py37/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/joanna/anaconda2/envs/py37/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/joanna/anaconda2/envs/py37/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/joanna/anaconda2/envs/py37/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/joanna/anaconda2/envs/py37/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'model_image' from '/Users/joanna/Desktop/Insight/CurlyHair/model_image.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import sys\n",
    "#sys.path.append('./flaskexample/')\n",
    "import importlib\n",
    "import model_image\n",
    "importlib.reload(model_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = glob('masked_images/twentyfour/*')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame(columns=['file_path', 'class', 'pred'])\n",
    "#new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "./hair_images/ad3f504ea1.jpg\n",
      "WARNING:tensorflow:From /Users/joanna/anaconda2/envs/py37/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/joanna/anaconda2/envs/py37/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "1\n",
      "./hair_images/aedd05030a.jpg\n",
      "2\n",
      "./hair_images/ad9dd80e0b.jpg\n",
      "3\n",
      "./hair_images/ba8eace3ed.jpg\n",
      "4\n",
      "./hair_images/b760763943.jpg\n",
      "5\n",
      "./hair_images/bade9cc0f8.jpg\n",
      "6\n",
      "./hair_images/adee46862b.jpg\n",
      "7\n",
      "./hair_images/ae9bf1b545.jpg\n",
      "8\n",
      "./hair_images/abe71ac7ce.jpg\n",
      "9\n",
      "./hair_images/b99133b264.jpg\n",
      "10\n",
      "./hair_images/b479986fa3.jpg\n",
      "11\n",
      "./hair_images/ac8ee58466.jpg\n",
      "12\n",
      "./hair_images/b731853146.jpg\n",
      "13\n",
      "./hair_images/aebdb060a6.jpg\n",
      "14\n",
      "./hair_images/ba40eddeb7.jpg\n",
      "15\n",
      "./hair_images/ae667cdca6.jpg\n",
      "16\n",
      "./hair_images/ba352ebdcf.jpg\n",
      "17\n",
      "./hair_images/ae12821717.jpg\n",
      "18\n",
      "./hair_images/ac6d04a5ae.jpg\n",
      "19\n",
      "./hair_images/ad79abe401.jpg\n",
      "20\n",
      "./hair_images/ba3a80e812.jpg\n",
      "21\n",
      "./hair_images/b82721765b.jpg\n",
      "22\n",
      "./hair_images/aeef5dafca.jpg\n",
      "23\n",
      "./hair_images/b219693927.jpg\n",
      "24\n",
      "./hair_images/aba085cdd4.jpg\n",
      "25\n",
      "./hair_images/af85ad64d0.jpg\n",
      "26\n",
      "./hair_images/ad104f856d.jpg\n",
      "27\n",
      "./hair_images/adabd2ab6c.jpg\n",
      "28\n",
      "./hair_images/ae1fac1f6c.jpg\n",
      "29\n",
      "./hair_images/aba2c6dec8.jpg\n",
      "30\n",
      "./hair_images/ae6ff7b72e.jpg\n",
      "31\n",
      "./hair_images/ac60be9a06.jpg\n",
      "32\n",
      "./hair_images/af6d0157d5.jpg\n",
      "33\n",
      "./hair_images/af3869bf9e.jpg\n",
      "34\n",
      "./hair_images/ad13f9db45.jpg\n",
      "35\n",
      "./hair_images/aed35d967c.jpg\n",
      "36\n",
      "./hair_images/acc80e2883.jpg\n",
      "37\n",
      "./hair_images/adffbc0185.jpg\n",
      "38\n",
      "./hair_images/b79000694c.jpg\n",
      "39\n",
      "./hair_images/baf852ff94.jpg\n",
      "40\n",
      "./hair_images/abe6719a16.jpg\n",
      "41\n",
      "./hair_images/ba82d228fe.jpg\n",
      "42\n",
      "./hair_images/ae9e6f788b.jpg\n",
      "43\n",
      "./hair_images/ba26501d9d.jpg\n",
      "44\n",
      "./hair_images/ab11675cbd.jpg\n",
      "45\n",
      "./hair_images/b660980ce6.jpg\n",
      "46\n",
      "./hair_images/ba1a4f0395.jpg\n",
      "47\n",
      "./hair_images/bad086df9f.jpg\n",
      "48\n",
      "./hair_images/af5cbd8505.jpg\n",
      "49\n",
      "./hair_images/af7e265eee.jpg\n",
      "50\n",
      "./hair_images/af03e4a24f.jpg\n",
      "51\n",
      "./hair_images/ad4fe11b1a.jpg\n",
      "52\n",
      "./hair_images/ad3ea93593.jpg\n",
      "53\n",
      "./hair_images/af0e7507ac.jpg\n",
      "54\n",
      "./hair_images/adaa3aab05.jpg\n",
      "55\n",
      "./hair_images/bab8a99e52.jpg\n",
      "56\n",
      "./hair_images/ac25c32913.jpg\n",
      "57\n",
      "./hair_images/ba9e8b4bf1.jpg\n",
      "58\n",
      "./hair_images/ac86dcfb68.jpg\n",
      "59\n",
      "./hair_images/afa96e460d.jpg\n",
      "60\n",
      "./hair_images/ac185f8d23.jpg\n",
      "61\n",
      "./hair_images/ac88b8273c.jpg\n",
      "62\n",
      "./hair_images/b94076ec61.jpg\n",
      "63\n",
      "./hair_images/ba9cb4bd1c.jpg\n",
      "64\n",
      "./hair_images/bab4f5764f.jpg\n",
      "65\n",
      "./hair_images/b942066829.jpg\n",
      "66\n",
      "./hair_images/af7f746998.jpg\n",
      "67\n",
      "./hair_images/af4801fe3d.jpg\n",
      "68\n",
      "./hair_images/abc963735a.jpg\n",
      "69\n",
      "./hair_images/ada300b10f.jpg\n",
      "70\n",
      "./hair_images/ac801f3da8.jpg\n",
      "71\n",
      "./hair_images/af40a4a57a.jpg\n",
      "72\n",
      "./hair_images/b573496d4c.jpg\n",
      "73\n",
      "./hair_images/adf8cc3527.jpg\n",
      "74\n",
      "./hair_images/ac9de5d62e.jpg\n",
      "75\n",
      "./hair_images/ae9252aec7.jpg\n",
      "76\n",
      "./hair_images/ac5a652052.jpg\n",
      "77\n",
      "./hair_images/af009648bd.jpg\n",
      "78\n",
      "./hair_images/b847075003.jpg\n",
      "79\n",
      "./hair_images/ad2e1eb35b.jpg\n",
      "80\n",
      "./hair_images/b18326789a.jpg\n",
      "81\n",
      "./hair_images/ac6b8e3b68.jpg\n",
      "82\n",
      "./hair_images/ae9215e09e.jpg\n",
      "83\n",
      "./hair_images/add9df728d.jpg\n",
      "84\n",
      "./hair_images/ba0d97ce7c.jpg\n",
      "85\n",
      "./hair_images/acaaf04c5f.jpg\n",
      "86\n",
      "./hair_images/ae665bb6ea.jpg\n",
      "87\n",
      "./hair_images/b260985a89.jpg\n",
      "88\n",
      "./hair_images/b5454391be.jpg\n",
      "89\n",
      "./hair_images/ad1378dac6.jpg\n",
      "90\n",
      "./hair_images/ace84a6075.jpg\n",
      "91\n",
      "./hair_images/add2435286.jpg\n",
      "92\n",
      "./hair_images/b331604b53.jpg\n",
      "93\n",
      "./hair_images/ac6e3de573.jpg\n",
      "94\n",
      "./hair_images/af7ed563da.jpg\n",
      "95\n",
      "./hair_images/abb4515501.jpg\n",
      "96\n",
      "./hair_images/abc6bd6f28.jpg\n",
      "97\n",
      "./hair_images/af8e738a3f.jpg\n",
      "98\n",
      "./hair_images/ac146be415.jpg\n",
      "99\n",
      "./hair_images/acd1f06ae3.jpg\n",
      "100\n",
      "./hair_images/b149786310.jpg\n",
      "101\n",
      "./hair_images/ae8b518b73.jpg\n",
      "102\n",
      "./hair_images/ae8a76df57.jpg\n",
      "103\n",
      "./hair_images/acf43856d9.jpg\n",
      "104\n",
      "./hair_images/af322520da.jpg\n",
      "105\n",
      "./hair_images/ad60f49b3e.jpg\n",
      "106\n",
      "./hair_images/ad85a678bd.jpg\n",
      "107\n",
      "./hair_images/ad43850a25.jpg\n",
      "108\n",
      "./hair_images/b314993929.jpg\n",
      "109\n",
      "./hair_images/ab31040b68.jpg\n",
      "110\n",
      "./hair_images/af536bb6f4.jpg\n",
      "111\n",
      "./hair_images/ba084de38d.jpg\n",
      "112\n",
      "./hair_images/ad7e83d5f7.jpg\n",
      "113\n",
      "./hair_images/b3239042d5.jpg\n",
      "114\n",
      "./hair_images/af2fdfc803.jpg\n",
      "115\n",
      "./hair_images/ad7388dbef.jpg\n",
      "116\n",
      "./hair_images/b597659a53.jpg\n",
      "117\n",
      "./hair_images/ae649fb786.jpg\n",
      "118\n",
      "./hair_images/ae7f082e7e.jpg\n",
      "119\n",
      "./hair_images/af3153757a.jpg\n",
      "120\n",
      "./hair_images/af1b3ebd7f.jpg\n",
      "121\n",
      "./hair_images/ad11bbd694.jpg\n",
      "122\n",
      "./hair_images/ae8b2c100c.jpg\n",
      "123\n",
      "./hair_images/ac38719e54.jpg\n",
      "124\n",
      "./hair_images/ae5beb2bbd.jpg\n",
      "125\n",
      "./hair_images/ad8a1b9a89.jpg\n",
      "126\n",
      "./hair_images/afbed0dd00.jpg\n",
      "127\n",
      "./hair_images/ae3f47089e.jpg\n",
      "128\n",
      "./hair_images/ab65181f07.jpg\n",
      "129\n",
      "./hair_images/bae6423e45.jpg\n",
      "130\n",
      "./hair_images/ad839b0de0.jpg\n",
      "131\n",
      "./hair_images/afa3c1f1ad.jpg\n",
      "132\n",
      "./hair_images/ade2fc7413.jpg\n",
      "133\n",
      "./hair_images/ad59d5427e.jpg\n",
      "134\n",
      "./hair_images/acdaf25faf.jpg\n",
      "135\n",
      "./hair_images/aced1abd0e.jpg\n",
      "136\n",
      "./hair_images/b793287bed.jpg\n",
      "137\n",
      "./hair_images/abbc165074.jpg\n",
      "138\n",
      "./hair_images/af5ac03a79.jpg\n",
      "139\n",
      "./hair_images/b4394694d0.jpg\n",
      "140\n",
      "./hair_images/ac4fc7b3e1.jpg\n",
      "141\n",
      "./hair_images/aeac59bb65.jpg\n",
      "142\n",
      "./hair_images/ade1c653ce.jpg\n",
      "143\n",
      "./hair_images/af152d0f1c.jpg\n",
      "144\n",
      "./hair_images/aeac7b59a5.jpg\n",
      "145\n",
      "./hair_images/af6b649810.jpg\n",
      "146\n",
      "./hair_images/ae7b6cc237.jpg\n",
      "147\n",
      "./hair_images/ada151ff66.jpg\n",
      "148\n",
      "./hair_images/b034133996.jpg\n",
      "149\n",
      "./hair_images/b651151efd.jpg\n",
      "150\n",
      "./hair_images/abf303b8a4.jpg\n",
      "151\n",
      "./hair_images/ac40760faa.jpg\n",
      "152\n",
      "./hair_images/ba955a2177.jpg\n",
      "153\n",
      "./hair_images/aded47c469.jpg\n",
      "154\n",
      "./hair_images/acbe149849.jpg\n",
      "155\n",
      "./hair_images/acf0f4242b.jpg\n",
      "156\n",
      "./hair_images/ba4081e926.jpg\n",
      "157\n",
      "./hair_images/ae713ba4a8.jpg\n",
      "158\n",
      "./hair_images/baba1b528f.jpg\n",
      "159\n",
      "./hair_images/ace13df7ae.jpg\n",
      "160\n",
      "./hair_images/b666054914.jpg\n",
      "161\n",
      "./hair_images/ac18d8055d.jpg\n",
      "162\n",
      "./hair_images/ab4803671c.jpg\n",
      "163\n",
      "./hair_images/ace6bd2395.jpg\n",
      "164\n",
      "./hair_images/b8642739a6.jpg\n",
      "165\n",
      "./hair_images/ba62caed26.jpg\n",
      "166\n",
      "./hair_images/ac999f970f.jpg\n",
      "167\n",
      "./hair_images/ad35296ff1.jpg\n",
      "168\n",
      "./hair_images/ac7c61d0c9.jpg\n",
      "169\n",
      "./hair_images/b663664a91.jpg\n",
      "170\n",
      "./hair_images/af8cc24c78.jpg\n",
      "171\n",
      "./hair_images/af9b6d64f5.jpg\n",
      "172\n",
      "./hair_images/ba069463d0.jpg\n",
      "173\n",
      "./hair_images/abc2cfe296.jpg\n",
      "174\n",
      "./hair_images/aca838f742.jpg\n",
      "175\n",
      "./hair_images/b98486ab0a.jpg\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i,im in enumerate(images):\n",
    "    print(i)\n",
    "    fil = './hair_images/'+im.split('/')[2]\n",
    "    print(fil)\n",
    "    \n",
    "    #row = img_df[img_df['file_path'].str.match(fil)]\n",
    "    #if row.empty:\n",
    "    #    count += 1\n",
    "    #    continue\n",
    "\n",
    "    label = model_image.predict_class(im)\n",
    "    #print(label)\n",
    "    \n",
    "    #ind = row.index[0]\n",
    "    #img_df['class'].loc[ind] = label[0]\n",
    "    #img_df['pred'].loc[ind] = label[1]\n",
    "    new_df = new_df.append({'file_path' :fil , 'class' :label[0], 'pred':label[1]} , ignore_index=True)\n",
    "    \n",
    "new_df.to_csv('image_urls_labels24.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
